{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-03T00:46:20.496751Z",
     "start_time": "2025-02-03T00:46:18.865Z"
    }
   },
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "import jieba\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T00:46:20.504711Z",
     "start_time": "2025-02-03T00:46:20.497757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def dictvec():\n",
    "    \"\"\"\n",
    "    字典数据抽取\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # 实例化\n",
    "    # sparse改为True,输出的是每个不为零位置的坐标，稀疏矩阵可以节省存储空间\n",
    "    #矩阵中存在大量的0，sparse存储只记录非零位置，节省空间的作用\n",
    "    #Vectorizer中文含义是矢量器的含义\n",
    "    dict1 = DictVectorizer(sparse=False)  # 把sparse改为True看看\n",
    "\n",
    "    #每个样本都是一个字典，有三个样本\n",
    "    # 调用fit_transform\n",
    "    data = dict1.fit_transform([{'city': '北京', 'temperature': 100},\n",
    "                                {'city': '上海', 'temperature': 60},\n",
    "                                {'city': '深圳', 'temperature': 30}])\n",
    "    test_data = [{'city': '北京', 'temperature': 80},\n",
    "                 {'city': '深圳', 'temperature': 40}]\n",
    "    print(data)\n",
    "    print('-' * 50)\n",
    "    # 字典中的一些类别数据，分别进行转换成特征\n",
    "    print(dict1.get_feature_names_out())\n",
    "    print('-' * 50)\n",
    "    # print(dict1.inverse_transform(data))  #去看每个特征代表的含义，逆转回去\n",
    "    x_test = dict1.transform(test_data)\n",
    "    print(x_test)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "dictvec()"
   ],
   "id": "da070d5e76103b89",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   1.   0. 100.]\n",
      " [  1.   0.   0.  60.]\n",
      " [  0.   0.   1.  30.]]\n",
      "--------------------------------------------------\n",
      "['city=上海' 'city=北京' 'city=深圳' 'temperature']\n",
      "--------------------------------------------------\n",
      "[[ 0.  1.  0. 80.]\n",
      " [ 0.  0.  1. 40.]]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T00:46:20.514559Z",
     "start_time": "2025-02-03T00:46:20.505717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def convec():\n",
    "    \n",
    "    vector = CountVectorizer(min_df=2)\n",
    "    \n",
    "    res = vector.fit_transform(\n",
    "        [\"life is  short,i like python life\",\n",
    "         \"life is too long,i dislike python\",\n",
    "         \"life is short\"]\n",
    "    )\n",
    "    \n",
    "    print(vector.get_feature_names_out())\n",
    "    print('-' * 50)\n",
    "    print(res)\n",
    "    print('-' * 50)\n",
    "    print(type(res))\n",
    "    print('-' * 50)\n",
    "    print(res.toarray())\n",
    "    print('-' * 50)\n",
    "    print(vector.inverse_transform(res))\n",
    "    \n",
    "convec()"
   ],
   "id": "f36f744a5ec189d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is' 'life' 'python' 'short']\n",
      "--------------------------------------------------\n",
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 10 stored elements and shape (3, 4)>\n",
      "  Coords\tValues\n",
      "  (0, 1)\t2\n",
      "  (0, 0)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 2)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 2)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 3)\t1\n",
      "--------------------------------------------------\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "--------------------------------------------------\n",
      "[[1 2 1 1]\n",
      " [1 1 1 0]\n",
      " [1 1 0 1]]\n",
      "--------------------------------------------------\n",
      "[array(['life', 'is', 'short', 'python'], dtype='<U6'), array(['life', 'is', 'python'], dtype='<U6'), array(['life', 'is', 'short'], dtype='<U6')]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T00:46:20.519556Z",
     "start_time": "2025-02-03T00:46:20.515561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cutword():\n",
    "    \"\"\"\n",
    "    通过jieba对中文进行分词\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    con1 = jieba.cut(\"今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。\")\n",
    "\n",
    "    con2 = jieba.cut(\"我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。\")\n",
    "\n",
    "    con3 = jieba.cut(\n",
    "        \"如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。\")\n",
    "\n",
    "    # 转换成列表\n",
    "    print(type(con1))\n",
    "    print('-' * 50)\n",
    "    # 把生成器转换成列表\n",
    "    content1 = list(con1)\n",
    "    content2 = list(con2)\n",
    "    content3 = list(con3)\n",
    "    print(content1)\n",
    "    print(content2)\n",
    "    print(content3)\n",
    "    # 把列表转换成字符串,每个词之间用空格隔开\n",
    "    print('-' * 50)\n",
    "    c1 = ' '.join(content1)\n",
    "    c2 = ' '.join(content2)\n",
    "    c3 = ' '.join(content3)\n",
    "\n",
    "    return c1, c2, c3"
   ],
   "id": "1dac25456ac013a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T00:46:21.039689Z",
     "start_time": "2025-02-03T00:46:20.519556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tdif():\n",
    "    tf = TfidfVectorizer(smooth_idf=True)\n",
    "    \n",
    "    a,b,c = cutword()\n",
    "    \n",
    "    data = tf.fit_transform([a,b,c])\n",
    "    \n",
    "    print(tf.get_feature_names_out())\n",
    "    print('-' * 50)\n",
    "    print(type(data))\n",
    "    print('-' * 50)\n",
    "    print(tf.inverse_transform(data))\n",
    "    print('-' * 50)\n",
    "    print(data.toarray())\n",
    "    \n",
    "tdif()"
   ],
   "id": "c22540489acde65c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache C:\\Users\\false\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.506 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今天', '很', '残酷', '，', '明天', '更', '残酷', '，', '后天', '很', '美好', '，', '但', '绝对', '大部分', '是', '死', '在', '明天', '晚上', '，', '所以', '每个', '人', '不要', '放弃', '今天', '。']\n",
      "['我们', '看到', '的', '从', '很', '远', '星系', '来', '的', '光是在', '几百万年', '之前', '发出', '的', '，', '这样', '当', '我们', '看到', '宇宙', '时', '，', '我们', '是', '在', '看', '它', '的', '过去', '。']\n",
      "['如果', '只用', '一种', '方式', '了解', '某样', '事物', '，', '你', '就', '不会', '真正', '了解', '它', '。', '了解', '事物', '真正', '含义', '的', '秘密', '取决于', '如何', '将', '其', '与', '我们', '所', '了解', '的', '事物', '相', '联系', '。']\n",
      "--------------------------------------------------\n",
      "['一种' '不会' '不要' '之前' '了解' '事物' '今天' '光是在' '几百万年' '发出' '取决于' '只用' '后天' '含义'\n",
      " '大部分' '如何' '如果' '宇宙' '我们' '所以' '放弃' '方式' '明天' '星系' '晚上' '某样' '残酷' '每个'\n",
      " '看到' '真正' '秘密' '绝对' '美好' '联系' '过去' '这样']\n",
      "--------------------------------------------------\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "--------------------------------------------------\n",
      "[array(['今天', '残酷', '明天', '后天', '美好', '绝对', '大部分', '晚上', '所以', '每个', '不要',\n",
      "       '放弃'], dtype='<U4'), array(['我们', '看到', '星系', '光是在', '几百万年', '之前', '发出', '这样', '宇宙', '过去'],\n",
      "      dtype='<U4'), array(['我们', '如果', '只用', '一种', '方式', '了解', '某样', '事物', '不会', '真正', '含义',\n",
      "       '秘密', '取决于', '如何', '联系'], dtype='<U4')]\n",
      "--------------------------------------------------\n",
      "[[0.         0.         0.21821789 0.         0.         0.\n",
      "  0.43643578 0.         0.         0.         0.         0.\n",
      "  0.21821789 0.         0.21821789 0.         0.         0.\n",
      "  0.         0.21821789 0.21821789 0.         0.43643578 0.\n",
      "  0.21821789 0.         0.43643578 0.21821789 0.         0.\n",
      "  0.         0.21821789 0.21821789 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.2410822  0.         0.\n",
      "  0.         0.2410822  0.2410822  0.2410822  0.         0.\n",
      "  0.         0.         0.         0.         0.         0.2410822\n",
      "  0.55004769 0.         0.         0.         0.         0.2410822\n",
      "  0.         0.         0.         0.         0.48216441 0.\n",
      "  0.         0.         0.         0.         0.2410822  0.2410822 ]\n",
      " [0.15698297 0.15698297 0.         0.         0.62793188 0.47094891\n",
      "  0.         0.         0.         0.         0.15698297 0.15698297\n",
      "  0.         0.15698297 0.         0.15698297 0.15698297 0.\n",
      "  0.1193896  0.         0.         0.15698297 0.         0.\n",
      "  0.         0.15698297 0.         0.         0.         0.31396594\n",
      "  0.15698297 0.         0.         0.15698297 0.         0.        ]]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T00:46:21.044510Z",
     "start_time": "2025-02-03T00:46:21.039689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def minmax():\n",
    "    \"\"\"\n",
    "    归一化处理\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    mm = MinMaxScaler(feature_range=(0,1))\n",
    "    data = mm.fit_transform([[90, 2, 10, 40], [60, 4, 15, 45], [75, 3, 13, 46]])\n",
    "    print(data)\n",
    "\n",
    "minmax()\n",
    "    "
   ],
   "id": "d8a0680547eb0d04",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.         0.         0.        ]\n",
      " [0.         1.         1.         0.83333333]\n",
      " [0.5        0.5        0.6        1.        ]]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T00:46:21.051865Z",
     "start_time": "2025-02-03T00:46:21.044510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sd():\n",
    "    sd = StandardScaler()\n",
    "    data = sd.fit_transform([[90, 2, 10, 40], [60, 4, 15, 45], [75, 3, 13, 46]])\n",
    "    print(data)\n",
    "    print(sd.mean_)\n",
    "    print(sd.var_)\n",
    "    data1 = sd.transform([[190, 22, 110, 404], [660, 46, 145, 455], [715, 376, 143, 416]])\n",
    "    print(data1)\n",
    "sd()"
   ],
   "id": "d626811b860d68a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.22474487 -1.22474487 -1.29777137 -1.3970014 ]\n",
      " [-1.22474487  1.22474487  1.13554995  0.50800051]\n",
      " [ 0.          0.          0.16222142  0.88900089]]\n",
      "[75.          3.         12.66666667 43.66666667]\n",
      "[150.           0.66666667   4.22222222   6.88888889]\n",
      "[[  9.38971068  23.27015256  47.36865497 137.28713729]\n",
      " [ 47.76504998  52.66402947  64.40190419 156.71815672]\n",
      " [ 52.25578118 456.82983703  63.42857566 141.85914186]]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T00:46:21.058900Z",
     "start_time": "2025-02-03T00:46:21.052867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def Simpleinput():\n",
    "    si = SimpleImputer(missing_values=np.nan,strategy='mean')\n",
    "    data = si.fit_transform([[90, np.nan, 10, 40], [60, 4, 15, 45], [75, 3, 13, 46]])\n",
    "    print(data)\n",
    "Simpleinput()"
   ],
   "id": "fc50c0827646bce8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[90.   3.5 10.  40. ]\n",
      " [60.   4.  15.  45. ]\n",
      " [75.   3.  13.  46. ]]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T00:47:18.488746Z",
     "start_time": "2025-02-03T00:47:18.484757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def var():\n",
    "    vt = VarianceThreshold(threshold=0.2)\n",
    "    # 方差门槛 方差过滤\n",
    "    data = vt.fit_transform([[0, 2, 0, 3],\n",
    "                              [0, 1, 4, 3],\n",
    "                              [0, 1, 1, 3]])\n",
    "    print(data)\n",
    "    print(vt.get_feature_names_out())\n",
    "    print(vt.get_support(True))\n",
    "var()\n",
    "    \n",
    "    "
   ],
   "id": "aa2fc01d1f98a42b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0]\n",
      " [1 4]\n",
      " [1 1]]\n",
      "['x1' 'x2']\n",
      "[1 2]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T01:46:10.539819Z",
     "start_time": "2025-02-03T01:46:10.535494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pca():\n",
    "    \"\"\"\n",
    "    主成分分析\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    p = PCA(n_components=0.99)\n",
    "    odata = np.array([[2, 8, 4, 5],\n",
    "                      [6, 3, 0, 8],\n",
    "                      [5, 4, 9, 1]])\n",
    "    print(np.var(odata, axis=0).sum())\n",
    "    data = p.fit_transform([[2, 8, 4, 5],\n",
    "                            [6, 3, 0, 8],\n",
    "                            [5, 4, 9, 1]])\n",
    "    \n",
    "    print(data)\n",
    "    print(np.var(data, axis=0).sum())\n",
    "    print('-' * 50)\n",
    "    print(p.explained_variance_ratio_)\n",
    "    # 计算data的方差占总方差的比例\n",
    "    print(p.explained_variance_ratio_.sum())\n",
    "pca()\n",
    "    \n",
    "    \n",
    "    "
   ],
   "id": "bce406b9c786861d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.333333333333336\n",
      "[[-1.28620952e-15  3.82970843e+00]\n",
      " [-5.74456265e+00 -1.91485422e+00]\n",
      " [ 5.74456265e+00 -1.91485422e+00]]\n",
      "29.333333333333332\n",
      "--------------------------------------------------\n",
      "[0.75 0.25]\n",
      "1.0\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# load和fetch",
   "id": "b6ccc38fca79692a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T06:06:18.446109Z",
     "start_time": "2025-02-03T06:06:18.128810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "from sklearn.datasets import load_iris, fetch_20newsgroups, fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score"
   ],
   "id": "2c3a6b16de00b752",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ec1d781471c85bb2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
